{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Domestic tourism and travelling post the national (India) lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  added / updated specs:\n",
      "    - beautifulsoup4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    beautifulsoup4-4.9.0       |           py36_0         165 KB  anaconda\n",
      "    ca-certificates-2020.1.1   |                0         132 KB  anaconda\n",
      "    certifi-2020.4.5.1         |           py36_0         159 KB  anaconda\n",
      "    openssl-1.1.1g             |       h7b6447c_0         3.8 MB  anaconda\n",
      "    soupsieve-2.0              |             py_0          33 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  beautifulsoup4     anaconda/linux-64::beautifulsoup4-4.9.0-py36_0\n",
      "  soupsieve          anaconda/noarch::soupsieve-2.0-py_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> anaconda::ca-certificates-2020.1.1-0\n",
      "  certifi            conda-forge::certifi-2020.4.5.1-py36h~ --> anaconda::certifi-2020.4.5.1-py36_0\n",
      "  openssl            conda-forge::openssl-1.1.1g-h516909a_0 --> anaconda::openssl-1.1.1g-h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2020.4.5.1   | 159 KB    | ##################################### | 100% \n",
      "soupsieve-2.0        | 33 KB     | ##################################### | 100% \n",
      "ca-certificates-2020 | 132 KB    | ##################################### | 100% \n",
      "openssl-1.1.1g       | 3.8 MB    | ##################################### | 100% \n",
      "beautifulsoup4-4.9.0 | 165 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  added / updated specs:\n",
      "    - geopy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    geographiclib-1.50         |             py_0          34 KB  conda-forge\n",
      "    geopy-1.22.0               |     pyh9f0ad1d_0          63 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          97 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  geographiclib      conda-forge/noarch::geographiclib-1.50-py_0\n",
      "  geopy              conda-forge/noarch::geopy-1.22.0-pyh9f0ad1d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates      anaconda::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi               anaconda::certifi-2020.4.5.1-py36_0 --> conda-forge::certifi-2020.4.5.1-py36h9f0ad1d_0\n",
      "  openssl               anaconda::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "geopy-1.22.0         | 63 KB     | ##################################### | 100% \n",
      "geographiclib-1.50   | 34 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda beautifulsoup4 --yes\n",
    "!conda install -c conda-forge geopy --yes \n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Gathering all touristic hotspots of the country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.1: Scraping Hotspot names, ratings and number of ratings from google travel using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of BeautifulSoup objects\n",
    "outdoors = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X#ttdm=26.112690_78.025407_5&ttdmf=%252Fm%252F081jv3'\n",
    "outdoors_soup = BeautifulSoup(requests.get(outdoors).text,'html5lib')\n",
    "\n",
    "artculture = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X&rf=ChwKGAoHL20vMGpqdxINQXJ0ICYgQ3VsdHVyZRAB#ttdm=22.319764_76.681641_4&ttdmf=%25252Fm%25252F0l8cb'\n",
    "artculture_soup=BeautifulSoup(requests.get(artculture).text,'html5lib')\n",
    "\n",
    "history = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X&rf=ChcKEwoIL20vMDNnM3cSB0hpc3RvcnkQAQ#ttdm=20.837795_74.517894_4&ttdmf=%25252525252Fm%25252525252F0l8cb'\n",
    "history_soup=BeautifulSoup(requests.get(history).text,'html5lib')\n",
    "\n",
    "beaches = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X&rf=ChcKEwoIL20vMGIzeXISB0JlYWNoZXMQAQ#ttdm=13.316684_73.965106_5&ttdmf=%2525252525252Fm%2525252525252F0l8cb'\n",
    "beaches_soup=BeautifulSoup(requests.get(beaches).text,'html5lib')\n",
    "\n",
    "museums = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X&rf=ChcKEwoIL20vMDljbXESB011c2V1bXMQAQ#ttdm=21.658970_77.971442_4&ttdmf=%252525252525252Fm%252525252525252F0l8cb'\n",
    "museums_soup=BeautifulSoup(requests.get(museums).text,'html5lib')\n",
    "\n",
    "amusparks = 'https://www.google.com/travel/things-to-do/see-all?g2lb=2502548%2C4215767%2C4258168%2C4260007%2C4270442%2C4274032%2C4291318%2C4305595%2C4306835%2C4317915%2C4328159%2C4329288%2C4333265%2C4358983%2C4366684%2C4367954%2C4369397%2C4372336%2C4373848%2C4380601%2C4270859%2C4284970%2C4291517%2C4316256%2C4356899&hl=en&gl=in&un=1&otf=1&dest_mid=%2Fm%2F03rk0&dest_state_type=sattd&dest_src=ts&tcfs=EgoKCC9tLzAzcmsw&sa=X&rf=CiAKHAoJL20vMDEwampyEg9BbXVzZW1lbnQgUGFya3MQAQ#ttdm=14.520034_75.642766_4&ttdmf=%25252525252525252Fm%25252525252525252F0l8cb'\n",
    "amusparks_soup=BeautifulSoup(requests.get(amusparks).text,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering all the names of the locations from the beatifulsoup objects (where rating exists)\n",
    "outdoors_names=[name.find('div',class_='skFvHc').text for name in outdoors_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "artculture_names=[name.find('div',class_='skFvHc').text for name in artculture_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "history_names=[name.find('div',class_='skFvHc').text for name in history_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "beaches_names=[name.find('div',class_='skFvHc').text for name in beaches_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "museums_names=[name.find('div',class_='skFvHc').text for name in museums_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "amusparks_names=[name.find('div',class_='skFvHc').text for name in amusparks_soup.find_all('div',class_='GwjAi')\\\n",
    "               if name.find('span',class_='KFi5wf') is not None]\n",
    "\n",
    "#Gathering all the ratings of the locations from the beatifulsoup objects (where rating exists)\n",
    "outdoors_ratings=[name.find('span',class_='KFi5wf').text for name in outdoors_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "artculture_ratings=[name.find('span',class_='KFi5wf').text for name in artculture_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "history_ratings=[name.find('span',class_='KFi5wf').text for name in history_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "beaches_ratings=[name.find('span',class_='KFi5wf').text for name in beaches_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "museums_ratings=[name.find('span',class_='KFi5wf').text for name in museums_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "amusparks_ratings=[name.find('span',class_='KFi5wf').text for name in amusparks_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "\n",
    "#Gathering all the 'number of ratings' of the locations from the beatifulsoup objects (where rating exists)\n",
    "outdoors_nofratings=[name.find('span',class_='jdzyld').text for name in outdoors_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "artculture_nofratings=[name.find('span',class_='jdzyld').text for name in artculture_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "history_nofratings=[name.find('span',class_='jdzyld').text for name in history_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "beaches_nofratings=[name.find('span',class_='jdzyld').text for name in beaches_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "museums_nofratings=[name.find('span',class_='jdzyld').text for name in museums_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]\n",
    "amusparks_nofratings=[name.find('span',class_='jdzyld').text for name in amusparks_soup.find_all('div',class_='GwjAi')\\\n",
    "                  if name.find('span',class_='KFi5wf') is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding all the individual lists\n",
    "all_names=outdoors_names+artculture_names+history_names+beaches_names+museums_names+amusparks_names\n",
    "all_ratings=outdoors_ratings+artculture_ratings+history_ratings+beaches_ratings+museums_ratings+amusparks_ratings\n",
    "all_nofratings=outdoors_nofratings+artculture_nofratings+history_nofratings+beaches_nofratings+museums_nofratings+amusparks_nofratings\n",
    "\n",
    "#Converting ratings to float and number of ratings to int\n",
    "all_ratings=[float(value) for value in all_ratings]\n",
    "all_nofratings=[int(value.split('(')[1].split(')')[0].replace(',','')) for value in all_nofratings]\n",
    "\n",
    "_dict = {'Name':all_names,'Rating':all_ratings,'Number of Ratings':all_nofratings}\n",
    "_df=pd.DataFrame(_dict)\n",
    "_df=_df.drop_duplicates()\n",
    "\n",
    "all_names=_df[\"Name\"].tolist()\n",
    "all_ratings=_df[\"Rating\"].tolist()\n",
    "all_nofratings=_df[\"Number of Ratings\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Using Google Geocoding API to gather the following data:\n",
    "1. Latitude & Longitude\n",
    "2. Type (what type of touristic destination is it?)\n",
    "3. State and District\n",
    "\n",
    "NOTE: <b>Not using Foursquare</b> because it is very limited in India. Google API is a better substitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Credentials\n",
    "google_key='AIzaSyBw44XUyvcfktseD-Ga-v8DHNPH38ozLlA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude=[]\n",
    "longitude=[]\n",
    "state=[]\n",
    "district=[]\n",
    "typeofdest=[]\n",
    "for text in all_names:\n",
    "    _district=''\n",
    "    _state=''\n",
    "    _latitude=np.nan\n",
    "    _longitude=np.nan\n",
    "    _typeofdest=[]\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?address={},+Indial&key={}'.format(text,google_key)\n",
    "    res=requests.get(url).json()\n",
    "    if(not res['results']):\n",
    "        district.append('')\n",
    "        state.append('')\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)\n",
    "        typeofdest.append([])\n",
    "        continue\n",
    "    \n",
    "    for add in res[\"results\"][0]['address_components']:\n",
    "        \n",
    "        try: \n",
    "            if add['types'][0]=='administrative_area_level_2':\n",
    "                _district=add['long_name']\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        try: \n",
    "            if add['types'][0]=='administrative_area_level_1':\n",
    "                _state=add['long_name']\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "    #District\n",
    "    if (not _district):\n",
    "        district.append('')\n",
    "    else:\n",
    "        district.append(_district)\n",
    "    \n",
    "    #State\n",
    "    if (not _state):\n",
    "        state.append('')\n",
    "    else:\n",
    "        state.append(_state)\n",
    "            \n",
    "    _latitude=res[\"results\"][0]['geometry']['location']['lat']\n",
    "    _longitude=res[\"results\"][0]['geometry']['location']['lng']\n",
    "    _typeofdest=res['results'][0]['types']\n",
    "    \n",
    "    #Latitude\n",
    "    if (not _latitude):\n",
    "        latitude.append(np.nan)\n",
    "    else:\n",
    "        latitude.append(_latitude)\n",
    "    \n",
    "    #Longitude\n",
    "    if (not _longitude):\n",
    "        longitude.append(np.nan)\n",
    "    else:\n",
    "        longitude.append(_longitude)\n",
    "        \n",
    "    #Type of Touristic destination\n",
    "    if (not _typeofdest):\n",
    "        typeofdest.append([])\n",
    "    else:\n",
    "        typeofdest.append(_typeofdest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Name':all_names,'Latitude':latitude,'Longitude':longitude,'District':district,'State':\\\n",
    "                 state,'Ratings':all_ratings,'Number of Ratings':all_nofratings,'Type of Destination':\\\n",
    "                 typeofdest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean some districts in our dataframe\n",
    "df.loc[df['District']=='Chittaurgarh','District']='Chittorgarh'\n",
    "df.loc[df['District']=='Bellary','District']='Ballari'\n",
    "df.loc[df['District']=='Vilupuram','District']='Viluppuram'\n",
    "df.loc[df['District']=='Vishakhapatnam','District']='Visakhapatnam'\n",
    "df.loc[df['District']=='Andaman','District']='South Andaman'\n",
    "df.loc[df['District']=='Sabarkatha','District']='Sabarkantha'\n",
    "df.loc[df['District']=='Devbhoomi Dwarka','District']='Devbhumi Dwarka'\n",
    "df.loc[df['District']=='Pondicherry','District']='Puducherry'\n",
    "df.loc[df['State']=='Daman and Diu','State']='Dadra and Nagar Haveli and Daman and Diu'\n",
    "df.loc[df['District']=='Gurgaon','District']='Gurugram'\n",
    "df.loc[df['District']=='Sahibzada Ajit Singh Nagar','District']='Shaheed Bhagat Singh Nagar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows without district and with longitudes less than 68E caused by bad geocoding\n",
    "df = df[(df[\"State\"]!='')&(df[\"Longitude\"]>68)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning beach and fort types\n",
    "df.loc[df['Name'].str.upper().str.contains(\"BEACH\"),'Type of Destination'] = ['establishment','natural_feature','beach']\n",
    "df.loc[df['Name'].str.upper().str.contains(\"FORT\"),'Type of Destination'] = ['establishment','point_of_interest','tourist_attraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with faulty Destination Type\n",
    "count=0\n",
    "ind=[]\n",
    "for dest in df['Type of Destination']:\n",
    "    if(('political' in dest) or ('route' in dest) or (dest is None)):\n",
    "        ind.append(count)\n",
    "    count+=1\n",
    "df = df.drop(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping possible Duplicates\n",
    "df = df.drop_duplicates(subset='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to obtain districts of those touristic places without one, by using reverse geocoding\n",
    "_df=df[df['District']=='']\n",
    "count=0\n",
    "district=[]\n",
    "for lat,long in zip(_df['Latitude'],_df['Longitude']):\n",
    "    _district=[]\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?latlng={},{}&key={}'.format(lat,long,google_key)\n",
    "    res = requests.get(url).json()\n",
    "    for add in res[\"results\"][0]['address_components']:\n",
    "        \n",
    "        try: \n",
    "            if add['types'][0]=='administrative_area_level_2':\n",
    "                _district=add['long_name']\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "    #District\n",
    "    if (not _district):\n",
    "        district.append('')\n",
    "    else:\n",
    "        district.append(_district)\n",
    "df.loc[df['District']=='','District']=district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Getting additional data: \n",
    "#### 1. Number of Coronavirus cases in the State and District (https://www.covid19india.org/)\n",
    "#### 2. Population and Area of each district (https://en.wikipedia.org/wiki/List_of_districts_in_India)\n",
    "#### 3. Domestic Tourism footfall in each state in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API for District wise cases\n",
    "url = 'https://api.covid19india.org/v2/state_district_wise.json'\n",
    "res = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for getting data of all districts and states and the number of coronavirus cases for both\n",
    "all_districts=[]\n",
    "all_districts_cases=[]\n",
    "all_states=[]\n",
    "all_states_cases=[]\n",
    "_sum=0\n",
    "for i in res:\n",
    "    all_states.append(i['state'])\n",
    "    for j,_ in enumerate(i['districtData']):\n",
    "        all_districts.append(i['districtData'][j]['district'])\n",
    "        all_districts_cases.append(i['districtData'][j]['active'])\n",
    "        _sum+=i['districtData'][j]['active']\n",
    "    all_states_cases.append(_sum)\n",
    "    _sum=0\n",
    "\n",
    "#Changing some district name errors\n",
    "for n,dist in enumerate(all_districts):\n",
    "    if dist == 'Chamarajanagara':\n",
    "        all_districts[n]='Chamarajanagar'\n",
    "    if dist == 'East District':\n",
    "        all_districts[n]='East Sikkim'\n",
    "    if dist == 'West District':\n",
    "        all_districts[n]='West Sikkim'\n",
    "    if dist == 'Kancheepuram':\n",
    "        all_districts[n]='Kanchipuram'\n",
    "    if dist == 'Bengaluru Urban':\n",
    "        all_districts[n]='Bangalore Urban'\n",
    "    if dist == 'Bengaluru Rural':\n",
    "        all_districts[n]='Bangalore Rural'\n",
    "    if dist == 'Shivamogga':\n",
    "        all_districts[n]='Shimoga'\n",
    "    if dist == 'Gautam Buddha Nagar':\n",
    "        all_districts[n]='Gautam Buddh Nagar'\n",
    "    if dist == 'Shahid Bhagat Singh Nagar':\n",
    "        all_districts[n]='Shaheed Bhagat Singh Nagar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the number of cases in each touristic hotspot district\n",
    "district_cases = []\n",
    "_=pd.DataFrame(all_districts_cases,index=map(lambda x: x.upper(),all_districts))\n",
    "for dist in df['District'].str.upper():\n",
    "    try: \n",
    "        district_cases.append(_.loc[dist,0])\n",
    "    except:\n",
    "        district_cases.append(-1)\n",
    "\n",
    "state_cases = []\n",
    "_=pd.DataFrame(all_states_cases,index=map(lambda x: x.upper(),all_states))\n",
    "for state in df['State'].str.upper():\n",
    "    try: \n",
    "        state_cases.append(_.loc[state,0])\n",
    "    except:\n",
    "        state_cases.append(-1)        \n",
    "\n",
    "#Maharashtra and Bihar have a common district called Aurangabad, fixing this issue\n",
    "for n,state,dist in zip(range(df.shape[0]),df[\"State\"].str.upper(),df['District'].str.upper()):\n",
    "    if((state == 'MAHARASHTRA') and (dist == 'AURANGABAD')):\n",
    "                        district_cases[n]=529\n",
    "    elif ((state == 'BIHAR') and (dist == 'AURANGABAD')):\n",
    "                        district_cases[n]=8        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the columns\n",
    "df['District Cases']=district_cases\n",
    "df['State Cases']=state_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_districts_in_India'\n",
    "res=requests.get(url)\n",
    "soup=BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=soup.find_all('table',class_='wikitable sortable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "district=[]\n",
    "population=[]\n",
    "area=[]\n",
    "count=0\n",
    "for table in tables[1:]:\n",
    "    for row in table.find_all('tr'):\n",
    "        x=row.find_all('td')\n",
    "        if(x):\n",
    "            district.append(x[2].text.strip())\n",
    "            population.append(x[4].text.strip())\n",
    "            area.append(x[5].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,i in enumerate(population):\n",
    "    if '[' in i:\n",
    "        population[n]=i.split('[')[0].strip()\n",
    "    if ('–' in i or '-' in i):\n",
    "        population[n]=np.nan\n",
    "    try: \n",
    "        population[n]=int(population[n].replace(',',''))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,i in enumerate(area):\n",
    "    if '[' in i:\n",
    "        area[n]=i.split('[')[0].strip()\n",
    "    if ('–' in i or '-' in i):\n",
    "        area[n]=np.nan\n",
    "    try: \n",
    "        area[n]=float(area[n].replace(',',''))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually fixing some important data due to lack of proper data\n",
    "for n,i in enumerate(district):\n",
    "    if (i =='South East Delhi'):\n",
    "        population[n]=1500636\n",
    "        area[n]=102\n",
    "    if (i=='Mumbai City'):\n",
    "        district[n]='Mumbai'\n",
    "    if (i=='Chamarajnagar'):\n",
    "        district[n]='Chamarajanagar'\n",
    "    if (i=='Khandwa (East Nimar)'):\n",
    "        district[n]='Khandwa' \n",
    "    if (i=='Sabarkantha'):\n",
    "        district.append('Sabarkatha')\n",
    "        population.append(population[n])\n",
    "        area.append(area[n])\n",
    "    if (i=='Devbhoomi Dwarka'):\n",
    "        district[n]='Devbhumi Dwarka'\n",
    "    if (i=='Pondicherry'):\n",
    "        district[n]='Puducherry'\n",
    "    if (i=='Gurgaon'):\n",
    "        district[n]='Gurugram'\n",
    "    if (i=='Shahid Bhagat Singh Nagar'):\n",
    "        district[n]='Shaheed Bhagat Singh Nagar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       District  Population     Area\n",
      "73   Aurangabad   2511243.0   3303.0\n",
      "332  Aurangabad   3695928.0  10107.0\n"
     ]
    }
   ],
   "source": [
    "DF=pd.DataFrame({'District':district,'Population':population,'Area':area})\n",
    "#There are two Aruangabad's. Let's individually capture these values to fix our error later.\n",
    "print(DF.loc[DF['District']=='Aurangabad',:])\n",
    "#The first one is Bihar and the second is Maharashtra\n",
    "Aurangabad_Bihar_Pop=DF.iloc[73,1]\n",
    "Aurangabad_Bihar_Pop=DF.iloc[73,1]\n",
    "Aurangabad_Maha_Pop=DF.iloc[332,1]\n",
    "Aurangabad_Maha_Area=DF.iloc[332,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two lists to match with our data\n",
    "District_Population=[]\n",
    "District_Area=[]\n",
    "for dist,state in zip(df['District'].str.upper(),df['State'].str.upper()):\n",
    "    try:\n",
    "        District_Population.append(int(DF.loc[(DF['District'].str.upper()==dist),'Population']))\n",
    "    except:\n",
    "        if (dist=='AURANGABAD' and state=='MAHARASHTRA'):\n",
    "            District_Population.append(Aurangabad_Maha_Pop)\n",
    "        elif (dist=='AURANGABAD' and state=='BIHAR'):\n",
    "            District_Population.append(Aurangabad_Bihar_Pop)\n",
    "        else :\n",
    "            District_Population.append(np.nan)\n",
    "    try:\n",
    "        District_Area.append(float(DF.loc[(DF['District'].str.upper()==dist),'Area']))\n",
    "    except:\n",
    "        if (dist=='AURANGABAD' and state=='MAHARASHTRA'):\n",
    "            District_Area.append(Aurangabad_Maha_Area)\n",
    "        elif (dist=='AURANGABAD' and state=='BIHAR'):\n",
    "            District_Area.append(Aurangabad_Bihar_Area)\n",
    "        else: \n",
    "            District_Area.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['District Population']=District_Population\n",
    "df['District Area']=District_Area\n",
    "# Convert Data Type of population to int\n",
    "df['District Population']=df['District Population'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Clustering\n",
    "#### Trying k-means cluster and DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
